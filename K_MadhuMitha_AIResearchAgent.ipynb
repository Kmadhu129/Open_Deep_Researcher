{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjoQ0DlRTDfk"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet openai transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "try:\n",
        "\n",
        "    from openai import OpenAI\n",
        "except Exception:\n",
        "    OpenAI = None\n",
        "\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "except Exception:\n",
        "    AutoTokenizer = None\n",
        "    AutoModelForSeq2SeqLM = None\n",
        "\n",
        "\n",
        "try:\n",
        "    from getpass import getpass\n",
        "except Exception:\n",
        "    getpass = input\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def use_openai_generate(topic, client=None, model=\"gpt-4o-mini\"):\n",
        "    \"\"\"Generate 3 sub-questions and a summary using OpenAI Chat API.\n",
        "    Returns tuple: (subquestions_text, summary_text)\n",
        "    \"\"\"\n",
        "    if client is None:\n",
        "        raise RuntimeError(\"OpenAI client not provided\")\n",
        "\n",
        "\n",
        "    sub_qs_prompt = (\n",
        "        f\"You are a research assistant. Generate exactly 3 detailed, focused research sub-questions \"\n",
        "        f\"based on this topic: \\\"{topic}\\\". Number them 1., 2., 3. Keep each sub-question to one sentence.\"\n",
        "    )\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": sub_qs_prompt}],\n",
        "        temperature=0.2,\n",
        "        max_tokens=300,\n",
        "    )\n",
        "\n",
        "    sub_qs_text = resp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "    summary_prompt = (\n",
        "        \"Summarize the following three sub-questions into one concise paragraph that describes the research focus: \\n\"\n",
        "        + sub_qs_text\n",
        "    )\n",
        "\n",
        "    resp2 = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
        "        temperature=0.2,\n",
        "        max_tokens=200,\n",
        "    )\n",
        "\n",
        "    summary_text = resp2.choices[0].message.content.strip()\n",
        "    return sub_qs_text, summary_text\n",
        "\n",
        "\n",
        "def hf_fallback_generate(topic, hf_model_name=\"google/flan-t5-base\"):\n",
        "    \"\"\"Simple fallback using FLAN-T5 to perform the two steps.\n",
        "    This is NOT as reliable as OpenAI chat but works offline if the model is available locally.\n",
        "    Returns tuple: (subquestions_text, summary_text)\n",
        "    \"\"\"\n",
        "    if AutoTokenizer is None or AutoModelForSeq2SeqLM is None:\n",
        "        raise RuntimeError(\"Transformers not available. Install with: pip install transformers sentencepiece\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(hf_model_name)\n",
        "\n",
        "    prompt1 = (\n",
        "        f\"Generate exactly 3 detailed research sub-questions based on the topic: {topic}. Number them 1.,2.,3.\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt1, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
        "    sub_qs_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    prompt2 = \"Summarize the following sub-questions into a single concise paragraph: \\n\" + sub_qs_text\n",
        "    inputs2 = tokenizer(prompt2, return_tensors=\"pt\")\n",
        "    outputs2 = model.generate(**inputs2, max_new_tokens=128)\n",
        "    summary_text = tokenizer.decode(outputs2[0], skip_special_tokens=True)\n",
        "\n",
        "    return sub_qs_text, summary_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\" Mini AI Research Assistant \")\n",
        "    topic = input(\"Enter your research topic: \").strip()\n",
        "    if not topic:\n",
        "        print(\"No topic entered. Exiting.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    client = None\n",
        "    if api_key is None and OpenAI is not None:\n",
        "\n",
        "        print(\"OpenAI API key not found in environment.\")\n",
        "        k = getpass(\"Paste your OpenAI API key (will be hidden): \")\n",
        "        if k:\n",
        "            api_key = k.strip()\n",
        "            os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    if api_key and OpenAI is not None:\n",
        "        try:\n",
        "            client = OpenAI(api_key=api_key)\n",
        "            sub_qs, summary = use_openai_generate(topic, client=client)\n",
        "            print(\"\\nGenerated Sub-questions:\\n\")\n",
        "            print(sub_qs)\n",
        "            print(\"\\nSummary:\\n\")\n",
        "            print(summary)\n",
        "            return\n",
        "        except Exception as e:\n",
        "            print(\"OpenAI call failed:\", str(e))\n",
        "            print(\"Falling back to Hugging Face (if available)...\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        sub_qs, summary = hf_fallback_generate(topic)\n",
        "        print(\"\\nGenerated Sub-questions (HF fallback):\\n\")\n",
        "        print(sub_qs)\n",
        "        print(\"\\nSummary:\\n\")\n",
        "        print(summary)\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(\"Fallback failed:\", str(e))\n",
        "        print(\"Unable to complete the task. Make sure you installed required packages and provided an OpenAI API key.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WICd408iTfri",
        "outputId": "99342ece-815d-4abb-db27-7d8b3908da5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mini AI Research Assistant \n",
            "Enter your research topic: Impact of Internet on People\n",
            "\n",
            "Generated Sub-questions:\n",
            "\n",
            "1. How has the proliferation of social media platforms influenced interpersonal communication and relationships among different age groups?  \n",
            "2. In what ways has access to online information and resources affected individuals' decision-making processes regarding health and wellness?  \n",
            "3. What are the psychological effects of internet addiction on users, particularly in relation to their productivity and mental health?\n",
            "\n",
            "Summary:\n",
            "\n",
            "This research focuses on the multifaceted impact of digital technology on human behavior and relationships, examining how the rise of social media platforms has transformed interpersonal communication across various age groups, influenced decision-making in health and wellness through increased access to online information, and affected mental health and productivity due to internet addiction.\n"
          ]
        }
      ]
    }
  ]
}